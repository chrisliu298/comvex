# -*- coding: utf-8 -*-
"""scratchpad

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/notebooks/empty.ipynb
"""

# !pip install -q torchinfo

import torch
import torch.nn as nn
import torch.nn.functional as F
from torchinfo import summary

x = nn.Sequential(
    nn.Linear(3 * 32 * 32, 512),
    nn.Linear(512, 10),
)
summary(x, input_size=(1, 3 * 32 * 32))

model = nn.Sequential(
    nn.Linear(1_578_506, 512),
    nn.Linear(512, 1),
)
summary(model, input_size=(1, 1_578_506))


class Model(nn.Module):
    def __init__(self, embedding_dim):
        super().__init__()
        self.fc11 = nn.Linear(3 * 32 * 32 + 1, embedding_dim)
        self.fc12 = nn.Linear(512 + 1, embedding_dim)
        self.fc2 = nn.Linear(embedding_dim * 2, 512)
        self.fc = nn.Linear(512, 1)

    def forward(self, w1, b1, w2, b2):
        out1 = torch.mean(self.fc11(self.make_input(w1, b1)), dim=0)
        out2 = torch.mean(self.fc12(self.make_input(w2, b2)), dim=0)
        print(out1.shape, out2.shape)
        concat = torch.cat([out1, out2])
        out = F.relu(self.fc2(concat))
        out = self.fc(out)
        return out

    def make_input(self, weight, bias):
        return torch.hstack([weight, bias.view(-1, 1)])


inputs = list(x.state_dict().values())
model = Model(32)
summary(model, input_data=inputs)
